// src/models/ChatbotModel.ts

import { GoogleGenerativeAI } from "@google/generative-ai";
import { parseIntent, handleIntent } from "./intent";

const client = new GoogleGenerativeAI(process.env.GEMINI_API_KEY!);
const MODEL_NAME = "gemini-2.0-flash";

const SYSTEM_PROMPT = `
Voc√™ √© **Bia**, assistente virtual ADMINISTRATIVA da Biblioteca.

IDENTIDADE:
- Voc√™ fala SEMPRE com um administrador do sistema, nunca com um usu√°rio comum.
- Nunca diga frases do tipo "isso √© s√≥ para administradores" ou "voc√™ n√£o tem acesso".
- Trate quem fala com voc√™ como algu√©m interno, de confian√ßa, que gerencia a biblioteca.

PERSONALIDADE:
- Fala de forma natural, leve e humana.
- Acolhedora, simp√°tica e pr√≥xima, como uma colega de trabalho.
- Usa 1¬™ pessoa: "eu", "me conta", "te ajudo", "podemos fazer".
- Pode usar emojis de vez em quando.
- N√£o seja rob√≥tica, evite respostas engessadas ou extremamente formais.

COMPORTAMENTO GERAL:
- Responde qualquer tipo de pergunta: livros, usu√°rios, empr√©stimos, sistema, tecnologia, d√∫vidas aleat√≥rias, cumprimentos (bom dia, boa noite, tudo bem?), at√© curiosidades gerais.
- Quando identificar algo relacionado ao sistema da biblioteca, responda de forma objetiva e √∫til.
- Quando for papo leve, responda de forma descontra√≠da, mas sempre respeitosa.
- Se n√£o souber algo com certeza, seja honesta, mas tente ainda assim orientar, sugerir caminhos ou hip√≥teses.

MEM√ìRIA DE CONTEXTO (curto prazo):
- Considere sempre:
  - √∫ltimo assunto falado
  - √∫ltima inten√ß√£o (contar livros, listar, etc.)
  - humor aproximado do administrador (bom, neutro, cansado)
- Use isso para tornar a resposta mais cont√≠nua e natural, sem for√ßar.

TOM:
- Profissional, mas amig√°vel.
- Enxuto, mas n√£o seco.
- Voc√™ pode perguntar de volta quando fizer sentido, para aprofundar o contexto.
`;

type Mood = "positivo" | "neutro" | "cansado";

interface MemoryState {
    lastTopic?: string;
    lastIntent?: string;
    mood?: Mood;
}

const memory: MemoryState = {};

function updateMemory(patch: Partial<MemoryState>) {
    Object.assign(memory, patch);
}

function getMemory(): MemoryState {
    return { ...memory };
}

function smalltalkHumanized(msg: string): string {
    const text = msg.toLowerCase();

    if (text.includes("bom dia")) {
        return "Bom dia! üòÑ J√° tomou um caf√© enquanto cuida da biblioteca hoje?";
    }
    if (text.includes("boa tarde")) {
        return "Boa tarde! ‚òï Como est√£o os empr√©stimos por a√≠?";
    }
    if (text.includes("boa noite")) {
        return "Boa noite! üåô Se quiser, posso te ajudar a encerrar o dia mais tranquilo.";
    }
    if (text.includes("tudo bem") || text.includes("como voc√™ est√°")) {
        return "T√¥ bem sim, obrigada por perguntar ü§ç E voc√™, como t√° por a√≠ no painel da biblioteca?";
    }

    const respostas = [
        "Oi! Que bom falar com voc√™ üòä Em que parte da biblioteca quer focar agora?",
        "E a√≠! üëã O que voc√™ quer fazer no sistema hoje?",
        "Ol√°aa! üíõ Me conta, vamos mexer com livros, usu√°rios ou empr√©stimos?",
        "Oi! T√¥ aqui contigo, √© s√≥ mandar o que voc√™ precisa üòâ",
    ];

    return respostas[Math.floor(Math.random() * respostas.length)] ?? "Oi! üëã Como posso te ajudar hoje?";
}

function isSmalltalk(msg: string): boolean {
    return /^(oi|ol√°|ola|bom dia|boa tarde|boa noite|e a√≠|ea√≠|tudo bem|como voc√™ est√°)/i.test(
        msg.trim()
    );
}


function generateFollowUp(intentName: string | undefined): string {
    if (!intentName) return "";

    const map: Record<string, string[]> = {
        COUNT_BOOKS: [
            "Se quiser, eu posso listar alguns livros espec√≠ficos tamb√©m.",
            "Quer que eu filtre por dispon√≠veis, autor ou categoria?",
        ],
        LIST_BOOKS: [
            "Se quiser refinar, me fala t√≠tulo, autor ou alguma palavra-chave üìö",
            "Posso te mostrar s√≥ os dispon√≠veis ou s√≥ os mais recentes, se preferir.",
        ],
        LIST_AVAILABLE_BOOKS: [
            "Se voc√™ quiser, posso te ajudar a decidir quais priorizar nos empr√©stimos.",
            "Quer que eu cruze isso com usu√°rios que mais pegam livros?",
        ],
        COUNT_USERS: [
            "Se fizer sentido, posso te ajudar a pensar em a√ß√µes para engajar mais leitores üòâ",
            "Se quiser, posso focar em empr√©stimos ativos desses usu√°rios.",
        ],
        COUNT_ACTIVE_LOANS: [
            "Quer que eu te lembre de olhar os atrasados depois?",
            "Se quiser, posso te ajudar a pensar em estrat√©gias para reduzir atrasos.",
        ],
        NAVIGATE: [
            "Se alguma tela estiver confusa, me conta que eu te ajudo a pensar melhorias.",
            "Se quiser, posso sugerir um fluxo pra agilizar seu dia a dia no painel.",
        ],
        SMALLTALK: [
            "E me conta, tem algum painel que voc√™ anda usando mais da biblioteca?",
            "Se quiser, te ajudo com alguma tarefa espec√≠fica agora üòâ",
        ],
    };

    const options = map[intentName] || [];
    if (!options.length) return "";

    const extra = options[Math.floor(Math.random() * options.length)] ?? "";
    return "\n\n" + extra;
}


export async function getAIResponse(message: string): Promise<string> {
    const userMsg = message.trim();
    const lower = userMsg.toLowerCase();

    try {
        if (isSmalltalk(userMsg)) {
            updateMemory({ mood: "positivo", lastTopic: userMsg, lastIntent: "SMALLTALK" });
            const resp = smalltalkHumanized(userMsg);
            return resp + generateFollowUp("SMALLTALK");
        }

        const intentResult = await parseIntent(userMsg);

        if (intentResult.confidence >= 0.6 && intentResult.intent !== "UNKNOWN") {
            const handled = await handleIntent(intentResult);

            if (handled && handled.type === "ok") {
                updateMemory({
                    lastIntent: intentResult.intent,
                    lastTopic: userMsg,
                });

                return handled.text + generateFollowUp(intentResult.intent);
            }
        }

        const mem = getMemory();
        let memoryContext = "";

        if (mem.lastTopic) {
            memoryContext += `√öltimo assunto que o admin comentou: "${mem.lastTopic}".\n`;
        }
        if (mem.lastIntent) {
            memoryContext += `√öltima inten√ß√£o identificada: ${mem.lastIntent}.\n`;
        }
        if (mem.mood) {
            memoryContext += `Humor aproximado do admin: ${mem.mood}.\n`;
        }

        const prompt = `
${SYSTEM_PROMPT}

CONTEXTO RECENTE:
${memoryContext || "Sem contexto relevante salvo no momento."}

Mensagem atual do administrador:
"${userMsg}"

Responda de forma natural, humana e √∫til.
Evite respostas muito longas, mas tamb√©m n√£o seja seca demais.
Se fizer sentido, fa√ßa 1 pergunta de continua√ß√£o no final.
`;

        const model = client.getGenerativeModel({ model: MODEL_NAME });
        const llm = await model.generateContent(prompt);
        const text = llm.response.text();

        updateMemory({ lastTopic: userMsg });

        return text;

    } catch (err) {
        console.error("AI Error (Bia):", err);
        return "Aconteceu alguma coisinha estranha do meu lado üòÖ Mas pode tentar de novo que eu continuo aqui contigo.";
    }
}
